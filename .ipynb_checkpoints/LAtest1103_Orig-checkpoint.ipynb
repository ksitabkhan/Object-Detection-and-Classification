{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class object_classification:\n",
    "    def __init__ (self):\n",
    "        self.IMAGE_SHAPE=(224,224)\n",
    "        self.classifier = tf.keras.Sequential([hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\", input_shape=self.IMAGE_SHAPE+(3,))])\n",
    "    \n",
    "    def TrainClassifier(self):\n",
    "        Image_labels = []\n",
    "        with open(\"ImageNet_Labels.txt\",\"r\") as f:\n",
    "            Image_labels = f.read().splitlines()\n",
    "    def SetDataDir(self):\n",
    "        data_dir=pathlib.Path(\".\\\\dataset\")\n",
    "        #list(data_dir.glob('*/*.jp*g'))[:5]\n",
    "        image_count = len(list(data_dir.glob('*/*g')))\n",
    "        print(\"Image count in Data set is \",image_count)\n",
    "        sedan = list(data_dir.glob('sedan/*'))\n",
    "        #sedan[:5]\n",
    "        suv = list(data_dir.glob('suv/*'))\n",
    "        #suv[:5]\n",
    "        car_dictionary = {\n",
    "            'suv' : list(data_dir.glob(\"suv/*\")),\n",
    "            'sedan': list(data_dir.glob(\"sedan/*\"))\n",
    "        }\n",
    "        car_dictionary['sedan']\n",
    "        car_label_dictionary = {\n",
    "            'sedan' : 0,\n",
    "            'suv' : 1\n",
    "        }\n",
    "        X, y = [], []\n",
    "\n",
    "        for car_name, images in car_dictionary.items():\n",
    "            for image in images:\n",
    "                img = cv2.imread(str(image))\n",
    "                resized_img = cv2.resize(img,IMAGE_SHAPE)\n",
    "                X.append(resized_img)\n",
    "                y.append(car_label_dictionary[car_name])\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        X_train_scaled= X_train/255\n",
    "        X_test_scaled = X_test/255\n",
    "        predicted = classifier.predict(np.array([X[0],X[1],X[2]]))\n",
    "        predicted = np.argmax(predicted, axis=1)\n",
    "        predicted\n",
    "        feature_extracter_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "        pretrained_model_without_top_layer = hub.KerasLayer(\n",
    "            feature_extracter_model, input_shape=(224, 224, 3), trainable=False)\n",
    "        num_of_cars = 2\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            pretrained_model_without_top_layer,\n",
    "            tf.keras.layers.Dense(num_of_cars)\n",
    "        ])\n",
    "\n",
    "        model.summary()\n",
    "        model.compile(  \n",
    "        optimizer=\"adam\",\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['acc'])\n",
    "\n",
    "        model.fit(X_train_scaled, y_train, epochs=5)\n",
    "        print(\"now evaluating the trained model\")\n",
    "        model.evaluate(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class object_detection(object_classification):\n",
    "    def __init__(self):\n",
    "        self.CONFIDENCE= 0.5\n",
    "        self.SCORE_THRESHOLD = 0.5\n",
    "        self.IOU_THRESHOLD = 0.5\n",
    "        self.configPath= \"C:\\\\Users\\khadija sitabkhan\\Documents\\Assignments\\Case_Studies\\yolov4-tiny.cfg\"\n",
    "        self.weightsPath= \"C:\\\\Users\\khadija sitabkhan\\Documents\\Assignments\\Case_Studies\\yolov4-tiny.weights\"\n",
    "        self.font_scale=1\n",
    "        self.thickness = 1\n",
    "        \n",
    "    def loadTinyYOLO(self):\n",
    "        self.labels = open(\"coco.names\").read().strip().split(\"\\n\")\n",
    "        self.colors = np.random.randint(0, 255, size=(len(self.labels), 3), dtype=\"uint8\")\n",
    "        self.net = cv2.dnn.readNetFromDarknet(self.configPath, self.weightsPath)\n",
    "        ln = self.net.getLayerNames()\n",
    "        ln = [ln[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "        return ln\n",
    "    def LoadVideo(self, ln):\n",
    "        self.car_counter=[]\n",
    "        video_file = \"assignment-clip.mp4\"\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        _, image = cap.read()\n",
    "        h, w = image.shape[:2]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "        out = cv2.VideoWriter(\"output.avi\", fourcc, 30.0, (w, h))\n",
    "        while True:\n",
    "            ret, image = cap.read()\n",
    "            if ret==True:\n",
    "                h, w = image.shape[:2]\n",
    "                blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "                self.net.setInput(blob)\n",
    "                start = time.perf_counter()\n",
    "                layer_outputs = self.net.forward(ln)\n",
    "                time_took = time.perf_counter() - start\n",
    "#                 print(\"Time took:\", time_took)\n",
    "                boxes, confidences, class_ids = [], [], []\n",
    "                for output in layer_outputs:\n",
    "                    # loop over each of the object detections\n",
    "                    for detection in output:\n",
    "                        # extract the class id (label) and confidence (as a probability) of\n",
    "                        # the current object detection\n",
    "                        scores = detection[5:]\n",
    "                        class_id = np.argmax(scores)\n",
    "                        confidence = scores[class_id]\n",
    "                        # discard weak predictions by ensuring the detected\n",
    "                        # probability is greater than the minimum probability\n",
    "                        if confidence > self.CONFIDENCE:\n",
    "                            # scale the bounding box coordinates back relative to the\n",
    "                            # size of the image, keeping in mind that YOLO actually\n",
    "                            # returns the center (x, y)-coordinates of the bounding\n",
    "                            # box followed by the boxes' width and height\n",
    "                            box = detection[:4] * np.array([w, h, w, h])\n",
    "                            (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                \n",
    "                            # use the center (x, y)-coordinates to derive the top and\n",
    "                            # and left corner of the bounding box\n",
    "                            x = int(centerX - (width / 2))\n",
    "                            y = int(centerY - (height / 2))\n",
    "                \n",
    "                            # update our list of bounding box coordinates, confidences,\n",
    "                            # and class IDs\n",
    "                            boxes.append([x, y, int(width), int(height)])\n",
    "                            confidences.append(float(confidence))\n",
    "                            class_ids.append(class_id)                \n",
    "                # perform the non maximum suppression given the scores defined before\n",
    "                idxs = cv2.dnn.NMSBoxes(boxes, confidences, self.SCORE_THRESHOLD, self.IOU_THRESHOLD)\n",
    "            \n",
    "                font_scale = 1\n",
    "                thickness = 1\n",
    "                car_per_frame=0\n",
    "                # ensure at least one detection exists\n",
    "                if len(idxs) > 0:\n",
    "                    \n",
    "                    # loop over the indexes we are keeping\n",
    "                    for i in idxs.flatten():\n",
    "                        # extract the bounding box coordinates\n",
    "                        x, y = boxes[i][0], boxes[i][1]\n",
    "                        w, h = boxes[i][2], boxes[i][3]\n",
    "                        # draw a bounding box rectangle and label on the image\n",
    "                        color = [int(c) for c in self.colors[class_ids[i]]]\n",
    "                        cv2.rectangle(image, (x, y), (x + w, y + h), color=color, thickness=thickness)\n",
    "                        text = f\"{self.labels[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "                        if (self.labels[class_ids[i]== \"car\"]):\n",
    "                            car_per_frame += 1\n",
    "#                         print(\"frame number is \",i,\" labels is \", self.labels[class_ids[i]])\n",
    "                        # calculate text width & height to draw the transparent boxes as background of the text\n",
    "                        (text_width, text_height) = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, fontScale=font_scale, thickness=thickness)[0]\n",
    "                        text_offset_x = x\n",
    "                        text_offset_y = y - 5\n",
    "                        box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height))\n",
    "                        overlay = image.copy()\n",
    "                        cv2.rectangle(overlay, box_coords[0], box_coords[1], color=color, thickness=cv2.FILLED)\n",
    "                        # add opacity (transparency to the box)\n",
    "                        image = cv2.addWeighted(overlay, 0.6, image, 0.4, 0)\n",
    "                        # now put the text (label: confidence %)\n",
    "                        cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            fontScale=font_scale, color=(0, 0, 0), thickness=thickness)\n",
    "#                     print(\"car_count is \", car_per_frame)\n",
    "                    self.car_counter.append(car_per_frame)\n",
    "                \n",
    "                out.write(image)\n",
    "                print(\"type(image) \", type(image))\n",
    "#                 cv2.imshow(\"image\", image)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    break\n",
    "            else:\n",
    "                print(\"car_counter is \", self.car_counter)\n",
    "                break    \n",
    "        \n",
    "#         \n",
    "#         cv2.destroyAllWindows()           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n",
      "type(image)  <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-ab601ca187f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mobj1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadTinyYOLO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mobj1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadVideo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mln\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-b6bf1707594e>\u001b[0m in \u001b[0;36mLoadVideo\u001b[1;34m(self, ln)\u001b[0m\n\u001b[0;32m     89\u001b[0m                         \u001b[0mtext_offset_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                         \u001b[0mbox_coords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_offset_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_offset_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext_offset_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext_width\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_offset_y\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtext_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                         \u001b[0moverlay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m                         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverlay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_coords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_coords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFILLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                         \u001b[1;31m# add opacity (transparency to the box)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obj1 = object_detection()\n",
    "ln = obj1.loadTinyYOLO()\n",
    "obj1.LoadVideo(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objClass = object_classification()\n",
    "objClass.TrainClassifier()\n",
    "objClass.SetDataDir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "arr=[1,1,2,3,4]\n",
    "arr.append(4)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"Image.jpg\")\n",
    "crop_img = img[y_min:y_max, x_min:x_max]\n",
    "cv2.imwrite(\"Object.jpg\",crop_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
